<style>
  body {
    font-family: Meiryo;
  }
</style>

# [実応用に即したプライバシー保護解析とセキュアデータ基盤](https://www.jst.go.jp/kisoken/crest/project/1111114/1111114_2021.html)
# [Privacy-Preserving Data Analysis and Secure Data Infrastructure for Real Applications](https://www.jst.go.jp/kisoken/crest/project/1111114/1111114_2021.html)

## 目標

不正侵入によるデータの漏洩, 正当なユーザによるデータの持ち出し, 解析結果からの(故意または過失による)個人の特定, などの懸念を払拭, 軽減する, システムソフトウェア(データ基盤, OS), データ解析(差分プライバシー, 連合学習), 実応用(医療データ, 軌跡データ活用)の研究を一体的に進め, 安全に積極的なデータ活用可能なSociety 5.0の実現に貢献することを目指します.

本プロジェクトはJST CREST [「基礎理論とシステム基盤技術の融合によるSociety 5.0のための基盤ソフトウェアの創出」](https://www.jst.go.jp/kisoken/crest/research_area/ongoing/bunya2021-2.html) 領域に採択されています.

## 体制

* 代表
 * [田浦健次朗](https://www.eidos.ic.i.u-tokyo.ac.jp/~tau/) 東京大学大学院 [情報理工学系研究科](https://www.i.u-tokyo.ac.jp/) [電子情報学専攻](https://www.i.u-tokyo.ac.jp/edu/course/ice/index.shtml) 
* 主たる研究分担者
 * [吉川正俊](https://www.db.soc.i.kyoto-u.ac.jp/~yoshikawa/) 京都大学大学院 [情報学研究科](https://www.i.kyoto-u.ac.jp/) [社会情報学専攻](https://www.soc.i.kyoto-u.ac.jp/)
 * 花岡昇平 東京大学 [附属病院](https://www.h.u-tokyo.ac.jp/) [放射線科](http://www.ut-radiology.umin.jp/) 
 * [塙敏博](https://www.cspp.cc.u-tokyo.ac.jp/hanawa/) 東京大学 [情報基盤センター](https://www.itc.u-tokyo.ac.jp/) 
 * [曹洋](https://yangcao88.github.io/) 北海道大学 [情報科学研究院](https://www.ist.hokudai.ac.jp/) [情報理工学部門](https://www.csit.ist.hokudai.ac.jp/)
* 共同研究者
 * ...

## 研究内容

![目標](img/goals.png "サンプル")

 * 医用データ(診断画像, 電子カルテ, レセプトデータ), 人や物の位置情報データなど, 有用なデータの多くが個人情報に基づいています
 * それらのデータは通常, 個人の識別に必要な情報は除去(仮名化, ハッシング)された上で提供され, かつ目的としても, 機械学習のモデル構築(例: 大量の正常画像, 病理画像を用いて, 画像診断のモデルを構築する)や, 統計的なデータの計算(例: 期間ごと, 施設ごとの利用者数を算出する)など, 個人情報を露呈しない用途で利用されることがほとんどです
 * そうであっても, 個人情報を元にした解析結果(機械学習モデルや統計データ)と, 個人に対する外部知識を組み合わせて個人の情報を特定しうることが知られており(...), 個人情報を妥当な目的で提供・利用するにあたっての障壁になっています
 * また現在用いられているファイルシステムやクラウドストレージではデータの提供 $=$ データの読み出し権限を与える, ということであり, 読み出されたデータのコピー・流出をシステムによって制御することが困難であり, これもまた, データ提供の障壁となっています
 * これらの問題を解決するためには, 読み出したデータを外部に流出(出力)できないことを保証するシステム, 個人の情報を特定しうる結果を出力できないことを保証するシステムなど新しいシステム技術が必要です
 * それと並立するアプローチとして, データそのものをより匿名化し, 個人情報の復元を不可能にするという方法もあります(K匿名化やローカル差分プライバシなど)が, データの有用性が著しく損なわれるという問題があります
 * 本研究ではこれらの問題を解決するために様々なレイヤ(基盤ソフトウェア, アルゴリズムの枠組み, 実応用)で取り組みます
 * 具体的には以下のモジュールに分解し, 連携して取り組みます(すべて進行中の研究であり, 目標やアプローチの変更がありえます)
 
 1. 管理者への信頼に依拠しないセキュアファイルシステム 
 1. プライバシー保護を強制・追跡可能なシステム機構
 1. 柔軟なプライバシー保護データ解析・機械学習 
 1. 医療・軌跡データ実応用での実証 
 
## 管理者への信頼に依拠しないセキュアファイルシステム 

## プライバシー保護を強制・追跡可能なシステム機構

 * このサブテーマでは, 個人情報を特定する解析結果の出力を抑止するためのシステム, --- 具体的にはプログラミング言語処理系 --- について研究する
 * 解析結果から個人情報を特定できないようにするための仕組みとして, (グローバル)差分プライバシーの枠組みを活用する
 * グローバル差分プライバシーの枠組みでは, 解析結果(多数の個人データの平均や和など)に確率的なノイズを加えることにより, 解析結果に対する特定個人の寄与を曖昧にし, 結果として解析結果から個人のデータが導き出されることを防ぐ
 * 目指す言語処理系では,
   * 個人の情報に基づくデータ(ファイルやデータベースからの入力を想定), およびそこから計算によって派生したデータは出力を禁止する,
   * 適切なグローバル差分プライバシー機構を通した(確率的なノイズを加えた)データのみ出力を許可する
ことによりプライバシー漏洩を防止する

## 柔軟なプライバシー保護データ解析・機械学習 
このサブテーマでは、以下の項目を研究する
*  データ保護型処理検証機能の開発．
多数の参加者が協調してパーソナルデータの解析を安全に実施するためには，各参加者から見て他者はすべて攻撃者であるという仮定に基づいたシステムを構築する必要がある．パーソナルデータの所有者がデータを他者に渡す前に自身で確率的な雑音を印加する局所差分プライバシー (local differential privacy)はそのための重要な要素技術である．しかし，逆に他者から見た場 合は所有者が攻撃者になり得るため，所有者は他者に対して自身の雑音印加が正しく実施されていることを検証可能にする必要がある．これは，データ内容を保護した上でそのデータに対する処理を第三者が外部から検証できるという一見矛盾した二つの要求を同時に満たす課題である．本研究実施項目では，暗号化された Randomized Response などの技術を用いてデータ保護型処理検証機能を開発する．
* データの意味を考慮した差分プライバシに基づくデータ解析．
差分プライバシーの定義は，敵対者が攪乱後のデータから真のデータを推測できる確率を最悪の ケースに基づいて定義し，その確率をプライバシー予算(ε)と呼ばれるパラメータによって制御可能としている．もとのデータに対してプライバシー保護機構を直列又は並列に複数回適用した際のプライバシー予算(ε)も応用に即したものでない場合も多く，結果として過剰な保護を行い解析精度が向上しないという問題につながる．そこで，本研究実施項目では，より実応用に即した意味に基づくプライバシー保護定義の確立とそれを満足するプライバシー保護機構の開発を行う．特に，時空間データやグラフデータなど非構造データにおけるプライバシー定義を考える．


## 医療・軌跡データ実応用での実証 
このサブテーマでは、以下の項目を研究する
* 医療データ実応用システムにおけるプライバシ保護システム・データ解析の検証．
本研究構想では小規模医院を含む多くの医療機関による連合学習 (federated learning)を医療 データへの実応用の重要な例と考える．たとえば，各医療機関とサーバの間で連合学習を行い画像診断の機械学習モデルを構築する場合，小規模医院のサンプル数が十分でない場合は局所モデルが不正確なものとなり得る．そのための連合学習アーキテクチャを開発する．たとえば，複数の小規模医院のデータを集めサーバとの間の中継を行う中継サーバを設け，中継サーバにおいて secure aggregation を行う手法などを開発する．
* 軌跡データ実応用システムにおける検証
  前述のサブトピック「柔軟なプライバシー保護データ解析・機械学習」で設計された連合学習と新しいプライバシー定義を，時空間データの文脈で検証し強化するである．時空間連合学習は，様々な位置情報サービスにおいて共有勾配のみを使用して有用なモデルを訓練する能力のため，最近集中的な研究を集めているである．複数の研究により，勾配を共有すると，勾配反転攻撃，メンバーシップ推論攻撃，画像やテキストに対するデータ抽出攻撃の対象となる可能性が示されているである．しかし，これらの攻撃は時空間連合学習においてまだ検証されていないである．加えて、既存の防御方法（例えば差分プライバシー）がこれらの攻撃を効果的に防げるかどうかは不明であるである．この研究項目では，攻撃と防御の両方の観点から，時空間連合学習における勾配からの攻撃によるプライバシー漏洩を評価することで，これらの問題に対処するである。
　We will verify and enhance the federated learning and new privacy definitions designed in the last sub-topic in the context of spatiotemporal data. Spatio-temporal federated learning has recently raised intensive studies due to its ability to train valuable models with only shared gradients in various location-based services. Several studies have shown that shared gradients may be subject to gradient inversion attacks, membership inference attacks, and data extraction attacks on images or texts. However, these attacks have yet to be validated in spatio-temporal federated learning. In addition, it is unclear whether existing defense methods, such as Differential Privacy, can prevent these attacks effectively. In this research item, we address these problems by evaluating the privacy leakage from gradients in spatiotemporal federated learning from two perspectives: attack and defense, respectively. 

